{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enable Amazon SageMaker Model Monitor\n",
    "\n",
    "* Update existing SageMaker Endpoint to enable Model Monitoring\n",
    "* Analyze the training dataset to generate a baseline constraint\n",
    "* Setup a MonitoringSchedule for monitoring deviations from the specified baseline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Enable real-time inference data capture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To enable data capture for monitoring the model data quality, you specify the new capture option called `DataCaptureConfig`. You can capture the request payload, the response payload or both with this configuration. The capture config applies to all variants. Please provide the Endpoint name in the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please fill in the following for enabling data capture\n",
    "endpoint_name = 'cmapss-XGBoostEndpoint-2021-01-14-14-57-30'\n",
    "s3_capture_upload_path = 's3://datalake-published-data-907317471167-us-east-1-pjkrtzr/model-monitor'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_bucket = f\"datalake-published-data-907317471167-us-east-1-pjkrtzr\"\n",
    "data_prefix = \"cmaps-ml\"\n",
    "train_prefix = \"split=train/year=2021\"\n",
    "eval_prefix = \"split=validation/year=2021\"\n",
    "data_bucket_path = f\"s3://{data_bucket}\"\n",
    "output_prefix = \"sagemaker/cmapss-xgboost\"\n",
    "snapshot_prefix = \"model_snapshots\"\n",
    "output_bucket_path = f\"s3://{data_bucket}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "role = sagemaker.get_execution_role()\n",
    "region = boto3.Session().region_name\n",
    "client = boto3.client(\"sagemaker\", region_name=region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------!!"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'EndpointName': 'cmapss-XGBoostEndpoint-2021-01-14-14-57-30',\n",
       " 'EndpointArn': 'arn:aws:sagemaker:us-east-1:907317471167:endpoint/cmapss-xgboostendpoint-2021-01-14-14-57-30',\n",
       " 'EndpointConfigName': 'cmapss-XGBoostEndpoint-2021-01-14-14-57-2021-01-14-18-28-22-124',\n",
       " 'ProductionVariants': [{'VariantName': 'AllTraffic',\n",
       "   'DeployedImages': [{'SpecifiedImage': '683313688378.dkr.ecr.us-east-1.amazonaws.com/sagemaker-xgboost:1.2-1',\n",
       "     'ResolvedImage': '683313688378.dkr.ecr.us-east-1.amazonaws.com/sagemaker-xgboost@sha256:cd8ab9e949aaa591ca914d9a4513d801e10e3fcc575f068154886b3c8930b7e8',\n",
       "     'ResolutionTime': datetime.datetime(2021, 1, 14, 18, 28, 25, 443000, tzinfo=tzlocal())}],\n",
       "   'CurrentWeight': 1.0,\n",
       "   'DesiredWeight': 1.0,\n",
       "   'CurrentInstanceCount': 1,\n",
       "   'DesiredInstanceCount': 1}],\n",
       " 'DataCaptureConfig': {'EnableCapture': True,\n",
       "  'CaptureStatus': 'Started',\n",
       "  'CurrentSamplingPercentage': 50,\n",
       "  'DestinationS3Uri': 's3://datalake-published-data-907317471167-us-east-1-pjkrtzr/model-monitor'},\n",
       " 'EndpointStatus': 'InService',\n",
       " 'CreationTime': datetime.datetime(2021, 1, 14, 14, 57, 30, 958000, tzinfo=tzlocal()),\n",
       " 'LastModifiedTime': datetime.datetime(2021, 1, 14, 18, 35, 46, 689000, tzinfo=tzlocal()),\n",
       " 'ResponseMetadata': {'RequestId': 'ee033cc0-3d08-4e7a-93f1-2b35c3015e61',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': 'ee033cc0-3d08-4e7a-93f1-2b35c3015e61',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '987',\n",
       "   'date': 'Thu, 14 Jan 2021 18:35:54 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sagemaker.model_monitor import DataCaptureConfig\n",
    "from sagemaker.predictor import Predictor as RealTimePredictor\n",
    "from sagemaker import session\n",
    "import boto3\n",
    "sm_session = session.Session(boto3.Session())\n",
    "\n",
    "# Change parameters as you would like - adjust sampling percentage, \n",
    "#  chose to capture request or response or both.\n",
    "#  Learn more from our documentation\n",
    "data_capture_config = DataCaptureConfig(enable_capture = True,\n",
    "                                        sampling_percentage=100,\n",
    "                                        destination_s3_uri=s3_capture_upload_path)\n",
    "\n",
    "# Now it is time to apply the new configuration and wait for it to be applied\n",
    "predictor = RealTimePredictor(endpoint_name=endpoint_name)\n",
    "predictor.update_data_capture_config(data_capture_config=data_capture_config)\n",
    "sm_session.wait_for_endpoint(endpoint=endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Model Monitor - Baselining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to collecting the data, SageMaker allows you to monitor and evaluate the data observed by the Endpoints. For this :\n",
    "1. We need to create a baseline with which we compare the realtime traffic against. \n",
    "1. Once a baseline is ready, we can setup a schedule to continously evaluate/compare against the baseline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constraint suggestion with baseline/training dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training dataset with which you trained the model is usually a good baseline dataset. Note that the training dataset's data schema and the inference dataset schema should exactly match (i.e. number and order of the features).\n",
    "\n",
    "Using our training dataset, we'll ask SageMaker to suggest a set of baseline constraints and generate descriptive statistics to explore the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline data uri: s3://datalake-published-data-907317471167-us-east-1-pjkrtzr/cmaps-ml/split=train/year=2021\n",
      "Baseline results uri: s3://datalake-published-data-907317471167-us-east-1-pjkrtzr/baseline_results\n"
     ]
    }
   ],
   "source": [
    "baseline_data_uri = f\"{data_bucket_path}/{data_prefix}/{train_prefix}\" \n",
    "baseline_results_uri = f\"{data_bucket_path}/baseline_results\" \n",
    "\n",
    "print('Baseline data uri: {}'.format(baseline_data_uri))\n",
    "print('Baseline results uri: {}'.format(baseline_results_uri))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a baselining job with the training dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the training data ready in S3, let's kick off a job to `suggest` constraints. `DefaultModelMonitor.suggest_baseline(..)` kicks off a `ProcessingJob` using a SageMaker provided Model Monitor container to generate the constraints. Please edit the configurations to fit your needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyathena\n",
      "  Downloading PyAthena-2.1.0-py3-none-any.whl (37 kB)\n",
      "Requirement already satisfied: botocore>=1.5.52 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from pyathena) (1.19.37)\n",
      "Requirement already satisfied: boto3>=1.4.4 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from pyathena) (1.16.37)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from boto3>=1.4.4->pyathena) (0.10.0)\n",
      "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from boto3>=1.4.4->pyathena) (0.3.3)\n",
      "Requirement already satisfied: botocore>=1.5.52 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from pyathena) (1.19.37)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from botocore>=1.5.52->pyathena) (1.25.11)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from botocore>=1.5.52->pyathena) (2.8.1)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from boto3>=1.4.4->pyathena) (0.10.0)\n",
      "Requirement already satisfied: six>=1.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from python-dateutil<3.0.0,>=2.1->botocore>=1.5.52->pyathena) (1.15.0)\n",
      "Requirement already satisfied: botocore>=1.5.52 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from pyathena) (1.19.37)\n",
      "Collecting tenacity>=4.1.0\n",
      "  Downloading tenacity-6.3.1-py2.py3-none-any.whl (36 kB)\n",
      "Requirement already satisfied: six>=1.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from python-dateutil<3.0.0,>=2.1->botocore>=1.5.52->pyathena) (1.15.0)\n",
      "Installing collected packages: tenacity, pyathena\n",
      "Successfully installed pyathena-2.1.0 tenacity-6.3.1\n",
      "\u001b[33mWARNING: You are using pip version 20.3; however, version 20.3.3 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/python3/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Collecting xgboost\n",
      "  Downloading xgboost-1.3.1-py3-none-manylinux2010_x86_64.whl (157.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 157.5 MB 22 kB/s s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from xgboost) (1.19.4)\n",
      "Requirement already satisfied: scipy in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from xgboost) (1.4.1)\n",
      "Requirement already satisfied: numpy in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from xgboost) (1.19.4)\n",
      "Installing collected packages: xgboost\n",
      "Successfully installed xgboost-1.3.1\n",
      "\u001b[33mWARNING: You are using pip version 20.3; however, version 20.3.3 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/python3/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: pyarrow in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (2.0.0)\n",
      "Requirement already satisfied: numpy>=1.14 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from pyarrow) (1.19.4)\n",
      "\u001b[33mWARNING: You are using pip version 20.3; however, version 20.3.3 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/python3/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: s3fs in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (0.4.2)\n",
      "Requirement already satisfied: fsspec>=0.6.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from s3fs) (0.6.2)\n",
      "Requirement already satisfied: botocore>=1.12.91 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from s3fs) (1.19.37)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from botocore>=1.12.91->s3fs) (0.10.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from botocore>=1.12.91->s3fs) (1.25.11)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from botocore>=1.12.91->s3fs) (2.8.1)\n",
      "Requirement already satisfied: six>=1.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from python-dateutil<3.0.0,>=2.1->botocore>=1.12.91->s3fs) (1.15.0)\n",
      "\u001b[33mWARNING: You are using pip version 20.3; however, version 20.3.3 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/python3/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install pyathena\n",
    "!pip install xgboost\n",
    "!pip install pyarrow\n",
    "!pip install s3fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow.parquet as pq\n",
    "import s3fs\n",
    "fs = s3fs.S3FileSystem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pq.ParquetDataset('s3://datalake-curated-datasets-907317471167-us-east-1-pjkrtzr/year=2021', filesystem=fs)\n",
    "table = dataset.read()\n",
    "df = table.to_pandas()\n",
    "df = df.sort_values(['unit_number', 'cycle'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['failure_cycle', 'cycle', 'op_1', 'op_2',\n",
    "       'op_3', 'sensor_measurement_1', 'sensor_measurement_2',\n",
    "       'sensor_measurement_3', 'sensor_measurement_4', 'sensor_measurement_5',\n",
    "       'sensor_measurement_6', 'sensor_measurement_7', 'sensor_measurement_8',\n",
    "       'sensor_measurement_9', 'sensor_measurement_10',\n",
    "       'sensor_measurement_11', 'sensor_measurement_12',\n",
    "       'sensor_measurement_13', 'sensor_measurement_14',\n",
    "       'sensor_measurement_15', 'sensor_measurement_16',\n",
    "       'sensor_measurement_17', 'sensor_measurement_18',\n",
    "       'sensor_measurement_19', 'sensor_measurement_20',\n",
    "       'sensor_measurement_21']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[features].to_csv('monitor_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "failure_cycle,cycle,op_1,op_2,op_3,sensor_measurement_1,sensor_measurement_2,sensor_measurement_3,sensor_measurement_4,sensor_measurement_5,sensor_measurement_6,sensor_measurement_7,sensor_measurement_8,sensor_measurement_9,sensor_measurement_10,sensor_measurement_11,sensor_measurement_12,sensor_measurement_13,sensor_measurement_14,sensor_measurement_15,sensor_measurement_16,sensor_measurement_17,sensor_measurement_18,sensor_measurement_19,sensor_measurement_20,sensor_measurement_21\n",
      "191,1,-0.0007,-0.0004,100.0,518.67,641.82,1589.7,1400.6,14.62,21.61,554.36,2388.06,9046.19,1.3,47.47,521.66,2388.02,8138.62,8.4195,0.03,392,2388,100.0,39.06,23.419\n",
      "320,1,42.0049,0.84,100.0,445.0,549.68,1343.43,1112.93,3.91,5.7,137.36,2211.86,8311.32,1.01,41.69,129.78,2387.99,8074.83,9.3335,0.02,330,2212,100.0,10.62,6.367\n",
      "148,1,34.9983,0.84,100.0,449.44,555.32,1358.61,1137.23,5.48,8.0,194.64,2222.65,8341.91,1.02,42.02,183.06,2387.72,8048.56,9.3461,0.02,334,2223,100.0,14.73,8.8071\n",
      "258,1,-0.0005,0.0004,100.0,518.67,642.36,1583.23,1396.84,14.62,21.61,553.97,2387.96,9062.17,1.3,47.3,522.31,2388.01,8145.32,8.4246,0.03,391,2388,100.0,39.11,23.3537\n",
      "190,2,0.0019,-0.0003,100.0,518.67,642.15,1591.82,1403.14,14.62,21.61,553.75,2388.04,9044.07,1.3,47.49,522.28,2388.07,8131.49,8.4318,0.03,392,2388,100.0,39.0,23.4236\n",
      "319,2,20.002,0.7002,100.0,491.19,606.07,1477.61,1237.5,9.35,13.61,332.1,2323.66,8713.6,1.07,43.94,312.59,2387.73,8046.13,9.1913,0.02,361,2324,100.0,24.37,14.6552\n",
      "147,2,41.9982,0.8408,100.0,445.0,549.9,1353.22,1125.78,3.91,5.71,138.51,2211.57,8303.96,1.02,42.2,130.42,2387.66,8072.3,9.3774,0.02,330,2212,100.0,10.41,6.2665\n",
      "257,2,0.0008,-0.0003,100.0,518.67,642.5,1584.69,1396.89,14.62,21.61,554.55,2388.0,9061.78,1.3,47.23,522.42,2388.03,8152.85,8.4403,0.03,392,2388,100.0,38.99,23.4491\n",
      "189,3,-0.0043,0.0003,100.0,518.67,642.35,1587.99,1404.2,14.62,21.61,554.26,2388.08,9052.94,1.3,47.27,522.42,2388.03,8133.23,8.4178,0.03,390,2388,100.0,38.95,23.3442\n"
     ]
    }
   ],
   "source": [
    "!head monitor_data.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Job Name:  baseline-suggestion-job-2021-01-14-22-22-39-114\n",
      "Inputs:  [{'InputName': 'baseline_dataset_input', 'S3Input': {'S3Uri': 's3://sagemaker-us-east-1-907317471167/model-monitor/baselining/baseline-suggestion-job-2021-01-14-22-22-39-114/input/baseline_dataset_input', 'LocalPath': '/opt/ml/processing/input/baseline_dataset_input', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}]\n",
      "Outputs:  [{'OutputName': 'monitoring_output', 'S3Output': {'S3Uri': 's3://datalake-published-data-907317471167-us-east-1-pjkrtzr/baseline_results', 'LocalPath': '/opt/ml/processing/output', 'S3UploadMode': 'EndOfJob'}}]\n",
      "................."
     ]
    }
   ],
   "source": [
    "from sagemaker.model_monitor import DefaultModelMonitor\n",
    "from sagemaker.model_monitor.dataset_format import DatasetFormat\n",
    "\n",
    "my_default_monitor = DefaultModelMonitor(\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type='ml.m5.large',\n",
    "    volume_size_in_gb=5,\n",
    "    max_runtime_in_seconds=3600,\n",
    ")\n",
    "\n",
    "my_default_monitor.suggest_baseline(\n",
    "    baseline_dataset='monitor_data.csv',\n",
    "    dataset_format=DatasetFormat.csv(header=True),\n",
    "    output_s3_uri=baseline_results_uri,\n",
    "    wait=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore the generated constraints and statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "baseline_job = my_default_monitor.latest_baselining_job\n",
    "schema_df = pd.io.json.json_normalize(baseline_job.baseline_statistics().body_dict[\"features\"])\n",
    "schema_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "constraints_df = pd.io.json.json_normalize(baseline_job.suggested_constraints().body_dict[\"features\"])\n",
    "constraints_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import math\n",
    "\n",
    "\n",
    "def do_predict(data, endpoint_name, content_type):\n",
    "    payload = \"\\n\".join(data)\n",
    "    response = runtime_client.invoke_endpoint(\n",
    "        EndpointName=endpoint_name, ContentType=content_type, Body=payload\n",
    "    )\n",
    "    result = response[\"Body\"].read()\n",
    "    result = result.decode(\"utf-8\")\n",
    "    result = result.split(\",\")\n",
    "    preds = [float((num)) for num in result]\n",
    "    preds = [math.ceil(num) for num in preds]\n",
    "    return preds\n",
    "\n",
    "\n",
    "def batch_predict(data, batch_size, endpoint_name, content_type):\n",
    "    items = len(data)\n",
    "    arrs = []\n",
    "\n",
    "    for offset in range(0, items, batch_size):\n",
    "        if offset + batch_size < items:\n",
    "            results = do_predict(data[offset : (offset + batch_size)], endpoint_name, content_type)\n",
    "            arrs.extend(results)\n",
    "        else:\n",
    "            arrs.extend(do_predict(data[offset:items], endpoint_name, content_type))\n",
    "        sys.stdout.write(\".\")\n",
    "    return arrs"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "!tail -n +2 monitor_data.csv | awk '{print \"-\"$0}' > model_monitor_bad_data.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('model_monitor_bad_data.csv', 'r') as f:\n",
    "    payload = f.read().strip()\n",
    "    inference_data = [line.strip() for line in payload.split(\"\\n\")][:10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runtime_client = boto3.client(\"runtime.sagemaker\", region_name=region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = batch_predict(inference_data, 1, endpoint_name, \"text/csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Enable continous monitoring\n",
    "\n",
    "We have collected the data above, here we proceed to analyze and monitor the data with MonitoringSchedules."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a schedule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are ready to create a model monitoring schedule for the Endpoint created earlier with the baseline resources (constraints and statistics)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.model_monitor import CronExpressionGenerator\n",
    "from time import gmtime, strftime\n",
    "\n",
    "mon_schedule_name = 'scheduled-monitor-report'\n",
    "s3_report_path = f\"{data_bucket_path}/monitoring-report\" \n",
    "\n",
    "my_default_monitor.create_monitoring_schedule(\n",
    "    monitor_schedule_name=mon_schedule_name,\n",
    "    endpoint_input=predictor.endpoint_name,\n",
    "    output_s3_uri=s3_report_path,\n",
    "    statistics=my_default_monitor.baseline_statistics(),\n",
    "    constraints=my_default_monitor.suggested_constraints(),\n",
    "    schedule_cron_expression=CronExpressionGenerator.hourly(),\n",
    "    enable_cloudwatch_metrics=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desc_schedule_result = my_default_monitor.describe_schedule()\n",
    "print('Schedule status: {}'.format(desc_schedule_result['MonitoringScheduleStatus']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desc_schedule_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mon_executions = my_default_monitor.list_executions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while len(mon_executions) == 0:\n",
    "    print(\"Waiting for the 1st execution to happen...\")\n",
    "    time.sleep(60)\n",
    "    mon_executions = my_default_monitor.list_executions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exe = mon_executions[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "constraints = exe.constraint_violations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "constraints.body_dict"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "notice": "Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved.  Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
