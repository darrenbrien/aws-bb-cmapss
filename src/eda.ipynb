{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pyathena\n",
    "\n",
    "!pip install pyarrow\n",
    "!pip install s3fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow.parquet as pq\n",
    "import s3fs\n",
    "import pandas as pd\n",
    "fs = s3fs.S3FileSystem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pq.ParquetDataset('s3://datalake-curated-datasets-907317471167-us-east-1-pjkrtzr/year=2021', filesystem=fs)\n",
    "table = dataset.read()\n",
    "df = table.to_pandas()\n",
    "df = df.sort_values(['unit_number', 'cycle'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 20))\n",
    "_ = df.groupby(['filename', 'unit_number']).cycle.max().plot.barh(ax=ax)\n",
    "_ = plt.axvline(x=df.groupby('unit_number').cycle.max().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.jointplot(x='op_1', y='failure_cycle', data=df.sample(1000), kind='reg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.jointplot(x='op_2', y='failure_cycle', data=df.sample(1000), kind='reg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.jointplot(x='op_3', y='failure_cycle', data=df.sample(10000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf = df.set_index(['filename', 'unit_number', 'cycle']).sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf.loc[['train_FD001.txt', ...]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(7, 3, figsize=(30, 40))\n",
    "axes = axes.ravel()\n",
    "for i, a in zip(range(1, 22), axes):\n",
    "    column = 'sensor_measurement_' + str(i)\n",
    "    _ = a.plot(ddf.loc[['train_FD001.txt', ...], column].unstack(level=[0, 1]).values, alpha=.05)\n",
    "    a.set_title(column)\n",
    "    a.set_xlabel('cycle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(7, 3, figsize=(30, 40))\n",
    "axes = axes.ravel()\n",
    "for i, a in zip(range(1, 22), axes):\n",
    "    column = 'sensor_measurement_' + str(i)\n",
    "    _ = a.plot(ddf.loc[['train_FD002.txt', ...], column].unstack(level=[0, 1]).values, alpha=.05)\n",
    "    a.set_title(column)\n",
    "    a.set_xlabel('cycle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(7, 3, figsize=(30, 40))\n",
    "axes = axes.ravel()\n",
    "for i, a in zip(range(1, 22), axes):\n",
    "    column = 'sensor_measurement_' + str(i)\n",
    "    _ = a.plot(ddf.loc[['train_FD003.txt', ...], column].unstack(level=[0, 1]).values, alpha=.05)\n",
    "    a.set_title(column)\n",
    "    a.set_xlabel('cycle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# the training data for 2 and 4 have 6 different operational settings and we can see how this effects the measurements here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(7, 3, figsize=(30, 40))\n",
    "axes = axes.ravel()\n",
    "for i, a in zip(range(1, 22), axes):\n",
    "    column = 'sensor_measurement_' + str(i)\n",
    "    _ = a.plot(ddf.loc[['train_FD004.txt', ...], column].unstack(level=[0, 1]).values, alpha=.05)\n",
    "    a.set_title(column)\n",
    "    a.set_xlabel('cycle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['cycle', 'op_1', 'op_2',\n",
    "       'op_3', 'sensor_measurement_1', 'sensor_measurement_2',\n",
    "       'sensor_measurement_3', 'sensor_measurement_4', 'sensor_measurement_5',\n",
    "       'sensor_measurement_6', 'sensor_measurement_7', 'sensor_measurement_8',\n",
    "       'sensor_measurement_9', 'sensor_measurement_10',\n",
    "       'sensor_measurement_11', 'sensor_measurement_12',\n",
    "       'sensor_measurement_13', 'sensor_measurement_14',\n",
    "       'sensor_measurement_15', 'sensor_measurement_16',\n",
    "       'sensor_measurement_17', 'sensor_measurement_18',\n",
    "       'sensor_measurement_19', 'sensor_measurement_20',\n",
    "       'sensor_measurement_21']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_train = df.unit_number % 3 != 0\n",
    "is_test = df.unit_number % 3 == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test = df.loc[is_train, features], df.loc[is_test, features]\n",
    "\n",
    "y_train, y_test = df.loc[is_train, 'failure_cycle'],  df.loc[is_test, 'failure_cycle']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls = RandomForestRegressor(n_jobs=-1, n_estimators=40, )\n",
    "\n",
    "cls = cls.fit(x_train, y_train)\n",
    "\n",
    "cls.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_squared_error(y_test, cls.predict(x_test), squared=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy = DummyRegressor()\n",
    "dummy = dummy.fit(x_train, y_train)\n",
    "mean_squared_error(y_test, dummy.predict(x_test), squared=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An ensemble based method outperforms a niave mean prediction by ~50% \n",
    "* Next steps apply Xgboost, gradient boosting generally outperforms random forest when tuned appropriately\n",
    "* this approach above validates the potencial value before we commit to building a sagemaker model, ie if there wasn't a margin over out \"dummy\" model then building a sagemaker model wouldn't probably be fruitful. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
